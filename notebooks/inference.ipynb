{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3ca1c3",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af540916",
   "metadata": {},
   "source": [
    "## Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a87282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "import shaft_force_sensing.models\n",
    "from shaft_force_sensing import ForceSensingDataset\n",
    "from shaft_force_sensing.models import (\n",
    "    LitSequenceModel,\n",
    "    LitTransformer,\n",
    "    LitLTC,\n",
    ")\n",
    "from shaft_force_sensing.evaluation import (\n",
    "    tb_to_numpy,\n",
    "    add_norm,\n",
    "    array_bais,\n",
    "    array_medfilt,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3bc7e",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430fa775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "max_epochs = 30\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af56e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model: LitLTC\n"
     ]
    }
   ],
   "source": [
    "save_path = Path(\"/scratch/pioneer/users/sxk2514/shaft_force_sensing/logs/20260216_225606\")\n",
    "\n",
    "model_cls = None\n",
    "for p in save_path.iterdir():\n",
    "    if p.name in shaft_force_sensing.models.__all__:\n",
    "        model_cls = p.name\n",
    "        print(\"Found model:\", model_cls)\n",
    "        break\n",
    "assert model_cls is not None, \"Model name not found in checkpoint directory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bddb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting checkpoint directory for hparams: /scratch/pioneer/users/sxk2514/shaft_force_sensing/logs/20260216_225606\n",
      "Found hparams: /scratch/pioneer/users/sxk2514/shaft_force_sensing/logs/20260216_225606/LitLTC/version_0/hparams.yaml\n",
      "Detected ablation config: No_Hex10\n",
      "Using 15 input columns (ablation=No_Hex10)\n"
     ]
    }
   ],
   "source": [
    "# Determine input columns from the checkpoint's hparams (ablation aware)\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from shaft_force_sensing.training.utils import get_input_cols_for_config\n",
    "\n",
    "# Default target columns\n",
    "t_cols = ['ati_fx', 'ati_fy', 'ati_fz']\n",
    "\n",
    "# `save_path` is the checkpoint directory selected by the user in the previous cell\n",
    "model_checkpoint_dir = save_path if 'save_path' in globals() else Path('../logs')\n",
    "print('Inspecting checkpoint directory for hparams:', model_checkpoint_dir)\n",
    "\n",
    "# Search for hparams.yaml (Lightning usually stores it under version_*/hparams.yaml)\n",
    "hparam_file = None\n",
    "if model_checkpoint_dir.exists():\n",
    "    # look for version folders first\n",
    "    for v in sorted(model_checkpoint_dir.glob('version_*')):\n",
    "        candidate = v / 'hparams.yaml'\n",
    "        if candidate.exists():\n",
    "            hparam_file = candidate\n",
    "            break\n",
    "    # fallback to recursive search\n",
    "    if hparam_file is None:\n",
    "        found = list(model_checkpoint_dir.rglob('hparams.yaml'))\n",
    "        if found:\n",
    "            hparam_file = found[0]\n",
    "\n",
    "if hparam_file is None:\n",
    "    print('No hparams.yaml found under', model_checkpoint_dir, '\\nUsing full input column set by default')\n",
    "    # Full set (fallback)\n",
    "    i_cols = [\n",
    "        'jaw_position', 'wrist_pitch_position', 'wrist_yaw_position', 'roll_position',\n",
    "        'wrist_pitch_velocity', 'wrist_yaw_velocity', 'jaw_velocity', 'roll_velocity',\n",
    "        'wrist_pitch_effort', 'wrist_yaw_effort', 'roll_effort',\n",
    "        'jaw_effort', 'insertion_effort', 'yaw_effort', 'pitch_effort',\n",
    "        'tx', 'ty', 'tz', 'fx', 'fy', 'fz'\n",
    "    ]\n",
    "else:\n",
    "    print('Found hparams:', hparam_file)\n",
    "    with open(hparam_file, 'r') as f:\n",
    "        h = yaml.safe_load(f)\n",
    "    config_name = h.get('ablation_config') or h.get('ablation') or h.get('input_config') or h.get('config') or 'Full'\n",
    "    print('Detected ablation config:', config_name)\n",
    "    i_cols = get_input_cols_for_config(config_name)\n",
    "\n",
    "print(f\"Using {len(i_cols)} input columns (ablation={config_name if hparam_file else 'Full'})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c69f0e",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b479cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = sorted(Path(\"../data\").rglob(\"*.csv\"))\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for p in data_paths:\n",
    "    groups[p.parent.name].append(p)\n",
    "\n",
    "test_paths = [lst[-1] for lst in groups.values()]\n",
    "train_paths = [p for p in data_paths if p not in test_paths]\n",
    "train_paths.pop(3);\n",
    "train_paths.pop(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497736",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c6a04",
   "metadata": {},
   "source": [
    "Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ba577db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: LitSequenceModel = eval(model_cls).load_from_checkpoint(\n",
    "    sorted(save_path.glob(\"best*.ckpt\"))[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be506a9d",
   "metadata": {},
   "source": [
    "Test set construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea769b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "golbal_scaler = StandardScaler()\n",
    "golbal_scaler.mean_ = model.data_mean.numpy(force=True)\n",
    "golbal_scaler.scale_ = model.data_std.numpy(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f17cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sets = dict()\n",
    "\n",
    "for p in tqdm(test_paths):\n",
    "    dataset = ForceSensingDataset(\n",
    "        p, i_cols, t_cols,\n",
    "        nomalizer=golbal_scaler)\n",
    "    test_sets[p.parent.name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b72899",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = {group: DataLoader(dset, batch_size=1000, shuffle=False)\n",
    "                for group, dset in test_sets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d66d5c",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d15dd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxk2514/.conda/envs/ltc311/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/sxk2514/.conda/envs/ltc311/lib/python3.11/site ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/sxk2514/.conda/envs/ltc311/lib/python3.11/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "/home/sxk2514/.conda/envs/ltc311/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [04:01<00:00,  0.25it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [05:31<00:00,  0.23it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [08:47<00:00,  0.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for group, loader in test_loaders.items():\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        save_path,\n",
    "        name='test',\n",
    "        version=group\n",
    "    )\n",
    "\n",
    "    Trainer(\n",
    "        logger=logger\n",
    "    ).test(\n",
    "        model=model,\n",
    "        dataloaders=loader\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "differentiable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
